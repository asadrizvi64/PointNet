{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "p5mNVaiiVvcp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "!pip install path.py;\n",
        "from path import Path\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
        "!unzip -q ModelNet10.zip\n",
        "\n",
        "path = Path(\"ModelNet10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep1aUsKWWFUQ",
        "outputId": "f33ccba4-0868-450d-8efc-0de3089c00b4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-01 13:29:01--  http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
            "Resolving 3dvision.princeton.edu (3dvision.princeton.edu)... 128.112.136.61\n",
            "Connecting to 3dvision.princeton.edu (3dvision.princeton.edu)|128.112.136.61|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 473402300 (451M) [application/zip]\n",
            "Saving to: ‘ModelNet10.zip.1’\n",
            "\n",
            "ModelNet10.zip.1    100%[===================>] 451.47M  75.2MB/s    in 6.2s    \n",
            "\n",
            "2022-06-01 13:29:07 (72.6 MB/s) - ‘ModelNet10.zip.1’ saved [473402300/473402300]\n",
            "\n",
            "replace ModelNet10/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/ModelNet10/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_off(file):\n",
        "    if 'OFF' != file.readline().strip():\n",
        "        raise('Not a valid OFF header')\n",
        "    n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
        "    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
        "    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
        "    return verts, faces\n",
        "    \n",
        "with open(path/\"bed/train/bed_0001.off\", 'r') as f:\n",
        "    mesh = read_off(f)"
      ],
      "metadata": {
        "id": "9LNQNgZ_Z8dH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verts, faces = mesh\n",
        "areas = np.zeros((len(faces)))\n",
        "verts = np.array(verts)\n",
        "\n",
        "def triangle_area(pt1, pt2, pt3):\n",
        "    side_a = np.linalg.norm(pt1 - pt2)\n",
        "    side_b = np.linalg.norm(pt2 - pt3)\n",
        "    side_c = np.linalg.norm(pt3 - pt1)\n",
        "    s = 0.5 * ( side_a + side_b + side_c)\n",
        "    return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
        "\n",
        "# we calculate areas of all faces in our mesh\n",
        "for i in range(len(areas)):\n",
        "    areas[i] = (triangle_area(verts[faces[i][0]],\n",
        "                              verts[faces[i][1]],\n",
        "                              verts[faces[i][2]]))"
      ],
      "metadata": {
        "id": "Ea3HFPWEWU4p"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3000\n",
        "\n",
        "sampled_faces = (random.choices(faces, \n",
        "                                weights=areas,\n",
        "                                k=k))\n",
        "\n",
        "def sample_point(pt1, pt2, pt3):\n",
        "\n",
        "    s, t = sorted([random.random(), random.random()])\n",
        "    f = lambda i: s * pt1[i] + (t-s) * pt2[i] + (1-t) * pt3[i]\n",
        "    return (f(0), f(1), f(2))\n",
        " \n",
        "pointcloud = np.zeros((k, 3))\n",
        "\n",
        "for i in range(len(sampled_faces)):\n",
        "    pointcloud[i] = (sample_point(verts[sampled_faces[i][0]],\n",
        "                                  verts[sampled_faces[i][1]],\n",
        "                                  verts[sampled_faces[i][2]]))"
      ],
      "metadata": {
        "id": "PfpuB-H8Wco9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "# rotation around z-axis\n",
        "theta = random.random() * 2. * math.pi # rotation angle\n",
        "rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
        "                       [ math.sin(theta),  math.cos(theta),    0],\n",
        "                       [0,                             0,      1]])\n",
        "\n",
        "rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
        "\n",
        "# add some noise\n",
        "noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
        "noisy_pointcloud = rot_pointcloud + noise"
      ],
      "metadata": {
        "id": "klyG1filXMg0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import scipy.spatial.distance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "       \n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "      \n",
        "      # initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      # add identity to the output\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix"
      ],
      "metadata": {
        "id": "z5uYKkhRXPY6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        self.conv1 = nn.Conv1d(3,64,1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64,128,1)\n",
        "        self.conv3 = nn.Conv1d(128,1024,1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "       \n",
        "   def forward(self, input):\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        return output, matrix3x3, matrix64x64"
      ],
      "metadata": {
        "id": "Xgjk7U6XXbew"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNet(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
        "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
        "        output = self.fc3(xb)\n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64"
      ],
      "metadata": {
        "id": "jOTvOLdAXhd3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs = outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
        "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3 = id3x3.cuda()\n",
        "        id64x64 = id64x64.cuda()\n",
        "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
        "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)"
      ],
      "metadata": {
        "id": "4DkjAoffXnIi"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1it-9eGXsxe",
        "outputId": "1b9a0d25-6d1c-46b3-9138-1b9b00e97b19"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pointnet = PointNet()\n",
        "pointnet.to(device);"
      ],
      "metadata": {
        "id": "A021abQhaFW7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "P1-z0R6BaNjx"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader=None,  epochs):\n",
        "    for epoch in range(epochs): \n",
        "        pointnet.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n",
        "\n",
        "            loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                    print('[Epoch: ', epoch + 1)\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        pointnet.eval()\n",
        "        correct = total = 0\n",
        "\n",
        "        # validation\n",
        "        if val_loader:\n",
        "            with torch.no_grad():\n",
        "                for data in val_loader:\n",
        "                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
        "                    outputs, __, __ = pointnet(inputs.transpose(1,2))\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "            val_acc = 100. * correct / total\n",
        "            print('Valid accuracy: %d %%' % val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "TygNTBBWaRS2",
        "outputId": "9c7bf940-3208-4164-f867-95a3c639b45f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-83-7442a857c565>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    torch.save(pointnet.state_dict(), \"save_\"+str(epoch)\".pth\")\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PointSampler(object):\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, int)\n",
        "        self.output_size = output_size\n",
        "    \n",
        "    def triangle_area(self, pt1, pt2, pt3):\n",
        "        side_a = np.linalg.norm(pt1 - pt2)\n",
        "        side_b = np.linalg.norm(pt2 - pt3)\n",
        "        side_c = np.linalg.norm(pt3 - pt1)\n",
        "        s = 0.5 * ( side_a + side_b + side_c)\n",
        "        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
        "\n",
        "    def sample_point(self, pt1, pt2, pt3):\n",
        "        # barycentric coordinates on a triangle\n",
        "        # https://mathworld.wolfram.com/BarycentricCoordinates.html\n",
        "        s, t = sorted([random.random(), random.random()])\n",
        "        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]\n",
        "        return (f(0), f(1), f(2))\n",
        "        \n",
        "    \n",
        "    def __call__(self, mesh):\n",
        "        verts, faces = mesh\n",
        "        verts = np.array(verts)\n",
        "        areas = np.zeros((len(faces)))\n",
        "\n",
        "        for i in range(len(areas)):\n",
        "            areas[i] = (self.triangle_area(verts[faces[i][0]],\n",
        "                                           verts[faces[i][1]],\n",
        "                                           verts[faces[i][2]]))\n",
        "            \n",
        "        sampled_faces = (random.choices(faces, \n",
        "                                      weights=areas,\n",
        "                                      cum_weights=None,\n",
        "                                      k=self.output_size))\n",
        "        \n",
        "        sampled_points = np.zeros((self.output_size, 3))\n",
        "\n",
        "        for i in range(len(sampled_faces)):\n",
        "            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]],\n",
        "                                                   verts[sampled_faces[i][1]],\n",
        "                                                   verts[sampled_faces[i][2]]))\n",
        "        \n",
        "        return sampled_points\n"
      ],
      "metadata": {
        "id": "TwYUtw-DbB-3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pointcloud = PointSampler(3000)((verts, faces))"
      ],
      "metadata": {
        "id": "6N9bsmMuai8p"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "        \n",
        "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "        return  norm_pointcloud"
      ],
      "metadata": {
        "id": "kV2Bxejqb5XN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_pointcloud = Normalize()(pointcloud)"
      ],
      "metadata": {
        "id": "MS1vI54ocAYw"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-STZ-lwscSQa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        return torch.from_numpy(pointcloud)"
      ],
      "metadata": {
        "id": "7huJ-rm3cDsn"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                PointSampler(1024),\n",
        "                                Normalize(),\n",
        "                                ToTensor()\n",
        "                              ])"
      ],
      "metadata": {
        "id": "wp9P0b4XcMFS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms()):\n",
        "        self.root_dir = root_dir\n",
        "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.transforms = transform if not valid else default_transforms()\n",
        "        self.valid = valid\n",
        "        self.files = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir/Path(category)/folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.off'):\n",
        "                    sample = {}\n",
        "                    sample['pcd_path'] = new_dir/file\n",
        "                    sample['category'] = category\n",
        "                    self.files.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "        verts, faces = read_off(file)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms((verts, faces))\n",
        "        return pointcloud\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.files[idx]['pcd_path']\n",
        "        category = self.files[idx]['category']\n",
        "        with open(pcd_path, 'r') as f:\n",
        "            pointcloud = self.__preproc__(f)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}"
      ],
      "metadata": {
        "id": "3vT-_TpWcZ40"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandRotation_z(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        theta = random.random() * 2. * math.pi\n",
        "        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
        "                               [ math.sin(theta),  math.cos(theta),    0],\n",
        "                               [0,                             0,      1]])\n",
        "        \n",
        "        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
        "        return  rot_pointcloud\n",
        "    \n",
        "class RandomNoise(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "\n",
        "        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
        "    \n",
        "        noisy_pointcloud = pointcloud + noise\n",
        "        return  noisy_pointcloud"
      ],
      "metadata": {
        "id": "Lf8ZsbYycgCz"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                PointSampler(1024),\n",
        "                                Normalize(),\n",
        "                                ToTensor()\n",
        "                              ])"
      ],
      "metadata": {
        "id": "1_bn54fyctWE"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms()):\n",
        "        self.root_dir = root_dir\n",
        "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.transforms = transform if not valid else default_transforms()\n",
        "        self.valid = valid\n",
        "        self.files = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir/Path(category)/folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.off'):\n",
        "                    sample = {}\n",
        "                    sample['pcd_path'] = new_dir/file\n",
        "                    sample['category'] = category\n",
        "                    self.files.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "        verts, faces = read_off(file)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms((verts, faces))\n",
        "        return pointcloud\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.files[idx]['pcd_path']\n",
        "        category = self.files[idx]['category']\n",
        "        with open(pcd_path, 'r') as f:\n",
        "            pointcloud = self.__preproc__(f)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}"
      ],
      "metadata": {
        "id": "fc0-4hCfczFw"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                    PointSampler(1024),\n",
        "                    Normalize(),\n",
        "                    RandRotation_z(),\n",
        "                    RandomNoise(),\n",
        "                    ToTensor()\n",
        "                    ])"
      ],
      "metadata": {
        "id": "Vv4Wey5sc4X8"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = PointCloudData(path, transform=train_transforms)\n",
        "valid_ds = PointCloudData(path, valid=True, folder='test', transform=train_transforms)"
      ],
      "metadata": {
        "id": "O-2zOpXZc8zA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_ds, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_ds, batch_size=64)"
      ],
      "metadata": {
        "id": "RNXD9Q0pdVU9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(pointnet, train_loader, valid_loader, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1P5mh6tdb2D",
        "outputId": "cc64d923-1488-49ee-c356-c602d1f30232"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 1, Batch:   10 /  125], loss: 0.807\n",
            "[Epoch: 1, Batch:   20 /  125], loss: 0.735\n",
            "[Epoch: 1, Batch:   30 /  125], loss: 0.734\n",
            "[Epoch: 1, Batch:   40 /  125], loss: 0.879\n",
            "[Epoch: 1, Batch:   50 /  125], loss: 0.774\n",
            "[Epoch: 1, Batch:   60 /  125], loss: 0.813\n",
            "[Epoch: 1, Batch:   70 /  125], loss: 0.665\n",
            "[Epoch: 1, Batch:   80 /  125], loss: 0.691\n",
            "[Epoch: 1, Batch:   90 /  125], loss: 0.722\n",
            "[Epoch: 1, Batch:  100 /  125], loss: 0.851\n",
            "[Epoch: 1, Batch:  110 /  125], loss: 0.639\n",
            "[Epoch: 1, Batch:  120 /  125], loss: 0.715\n",
            "Valid accuracy: 62 %\n",
            "[Epoch: 2, Batch:   10 /  125], loss: 0.627\n",
            "[Epoch: 2, Batch:   20 /  125], loss: 0.770\n",
            "[Epoch: 2, Batch:   30 /  125], loss: 0.714\n",
            "[Epoch: 2, Batch:   40 /  125], loss: 0.685\n",
            "[Epoch: 2, Batch:   50 /  125], loss: 0.683\n",
            "[Epoch: 2, Batch:   60 /  125], loss: 0.630\n",
            "[Epoch: 2, Batch:   70 /  125], loss: 0.669\n",
            "[Epoch: 2, Batch:   80 /  125], loss: 0.609\n",
            "[Epoch: 2, Batch:   90 /  125], loss: 0.717\n",
            "[Epoch: 2, Batch:  100 /  125], loss: 0.507\n",
            "[Epoch: 2, Batch:  110 /  125], loss: 0.459\n",
            "[Epoch: 2, Batch:  120 /  125], loss: 0.585\n",
            "Valid accuracy: 73 %\n",
            "[Epoch: 3, Batch:   10 /  125], loss: 0.557\n",
            "[Epoch: 3, Batch:   20 /  125], loss: 0.624\n",
            "[Epoch: 3, Batch:   30 /  125], loss: 0.499\n",
            "[Epoch: 3, Batch:   40 /  125], loss: 0.482\n",
            "[Epoch: 3, Batch:   50 /  125], loss: 0.709\n",
            "[Epoch: 3, Batch:   60 /  125], loss: 0.635\n",
            "[Epoch: 3, Batch:   70 /  125], loss: 0.658\n",
            "[Epoch: 3, Batch:   80 /  125], loss: 0.584\n",
            "[Epoch: 3, Batch:   90 /  125], loss: 0.561\n",
            "[Epoch: 3, Batch:  100 /  125], loss: 0.590\n",
            "[Epoch: 3, Batch:  110 /  125], loss: 0.561\n",
            "[Epoch: 3, Batch:  120 /  125], loss: 0.533\n",
            "Valid accuracy: 81 %\n",
            "[Epoch: 4, Batch:   10 /  125], loss: 0.516\n",
            "[Epoch: 4, Batch:   20 /  125], loss: 0.536\n",
            "[Epoch: 4, Batch:   30 /  125], loss: 0.624\n",
            "[Epoch: 4, Batch:   40 /  125], loss: 0.553\n",
            "[Epoch: 4, Batch:   50 /  125], loss: 0.620\n",
            "[Epoch: 4, Batch:   60 /  125], loss: 0.528\n",
            "[Epoch: 4, Batch:   70 /  125], loss: 0.485\n",
            "[Epoch: 4, Batch:   80 /  125], loss: 0.529\n",
            "[Epoch: 4, Batch:   90 /  125], loss: 0.518\n",
            "[Epoch: 4, Batch:  100 /  125], loss: 0.509\n",
            "[Epoch: 4, Batch:  110 /  125], loss: 0.544\n",
            "[Epoch: 4, Batch:  120 /  125], loss: 0.423\n",
            "Valid accuracy: 79 %\n",
            "[Epoch: 5, Batch:   10 /  125], loss: 0.419\n",
            "[Epoch: 5, Batch:   20 /  125], loss: 0.521\n",
            "[Epoch: 5, Batch:   30 /  125], loss: 0.421\n",
            "[Epoch: 5, Batch:   40 /  125], loss: 0.485\n",
            "[Epoch: 5, Batch:   50 /  125], loss: 0.467\n",
            "[Epoch: 5, Batch:   60 /  125], loss: 0.547\n",
            "[Epoch: 5, Batch:   70 /  125], loss: 0.471\n",
            "[Epoch: 5, Batch:   80 /  125], loss: 0.439\n",
            "[Epoch: 5, Batch:   90 /  125], loss: 0.537\n",
            "[Epoch: 5, Batch:  100 /  125], loss: 0.485\n",
            "[Epoch: 5, Batch:  110 /  125], loss: 0.427\n",
            "[Epoch: 5, Batch:  120 /  125], loss: 0.410\n",
            "Valid accuracy: 82 %\n",
            "[Epoch: 6, Batch:   10 /  125], loss: 0.529\n",
            "[Epoch: 6, Batch:   20 /  125], loss: 0.453\n",
            "[Epoch: 6, Batch:   30 /  125], loss: 0.420\n",
            "[Epoch: 6, Batch:   40 /  125], loss: 0.404\n",
            "[Epoch: 6, Batch:   50 /  125], loss: 0.411\n",
            "[Epoch: 6, Batch:   60 /  125], loss: 0.420\n",
            "[Epoch: 6, Batch:   70 /  125], loss: 0.462\n",
            "[Epoch: 6, Batch:   80 /  125], loss: 0.427\n",
            "[Epoch: 6, Batch:   90 /  125], loss: 0.443\n",
            "[Epoch: 6, Batch:  100 /  125], loss: 0.414\n",
            "[Epoch: 6, Batch:  110 /  125], loss: 0.493\n",
            "[Epoch: 6, Batch:  120 /  125], loss: 0.504\n",
            "Valid accuracy: 78 %\n",
            "[Epoch: 7, Batch:   10 /  125], loss: 0.518\n",
            "[Epoch: 7, Batch:   20 /  125], loss: 0.535\n",
            "[Epoch: 7, Batch:   30 /  125], loss: 0.488\n",
            "[Epoch: 7, Batch:   40 /  125], loss: 0.420\n",
            "[Epoch: 7, Batch:   50 /  125], loss: 0.348\n",
            "[Epoch: 7, Batch:   60 /  125], loss: 0.365\n",
            "[Epoch: 7, Batch:   70 /  125], loss: 0.417\n",
            "[Epoch: 7, Batch:   80 /  125], loss: 0.370\n",
            "[Epoch: 7, Batch:   90 /  125], loss: 0.403\n",
            "[Epoch: 7, Batch:  100 /  125], loss: 0.479\n",
            "[Epoch: 7, Batch:  110 /  125], loss: 0.411\n",
            "[Epoch: 7, Batch:  120 /  125], loss: 0.377\n",
            "Valid accuracy: 77 %\n",
            "[Epoch: 8, Batch:   10 /  125], loss: 0.414\n",
            "[Epoch: 8, Batch:   20 /  125], loss: 0.497\n",
            "[Epoch: 8, Batch:   30 /  125], loss: 0.443\n",
            "[Epoch: 8, Batch:   40 /  125], loss: 0.489\n",
            "[Epoch: 8, Batch:   50 /  125], loss: 0.498\n",
            "[Epoch: 8, Batch:   60 /  125], loss: 0.401\n",
            "[Epoch: 8, Batch:   70 /  125], loss: 0.515\n",
            "[Epoch: 8, Batch:   80 /  125], loss: 0.361\n",
            "[Epoch: 8, Batch:   90 /  125], loss: 0.389\n",
            "[Epoch: 8, Batch:  100 /  125], loss: 0.421\n",
            "[Epoch: 8, Batch:  110 /  125], loss: 0.483\n",
            "[Epoch: 8, Batch:  120 /  125], loss: 0.349\n",
            "Valid accuracy: 82 %\n",
            "[Epoch: 9, Batch:   10 /  125], loss: 0.335\n",
            "[Epoch: 9, Batch:   20 /  125], loss: 0.314\n",
            "[Epoch: 9, Batch:   30 /  125], loss: 0.401\n",
            "[Epoch: 9, Batch:   40 /  125], loss: 0.288\n",
            "[Epoch: 9, Batch:   50 /  125], loss: 0.402\n",
            "[Epoch: 9, Batch:   60 /  125], loss: 0.371\n",
            "[Epoch: 9, Batch:   70 /  125], loss: 0.464\n",
            "[Epoch: 9, Batch:   80 /  125], loss: 0.501\n",
            "[Epoch: 9, Batch:   90 /  125], loss: 0.426\n",
            "[Epoch: 9, Batch:  100 /  125], loss: 0.421\n",
            "[Epoch: 9, Batch:  110 /  125], loss: 0.446\n",
            "[Epoch: 9, Batch:  120 /  125], loss: 0.365\n",
            "Valid accuracy: 84 %\n",
            "[Epoch: 10, Batch:   10 /  125], loss: 0.409\n",
            "[Epoch: 10, Batch:   20 /  125], loss: 0.413\n",
            "[Epoch: 10, Batch:   30 /  125], loss: 0.330\n",
            "[Epoch: 10, Batch:   40 /  125], loss: 0.356\n",
            "[Epoch: 10, Batch:   50 /  125], loss: 0.370\n",
            "[Epoch: 10, Batch:   60 /  125], loss: 0.402\n",
            "[Epoch: 10, Batch:   70 /  125], loss: 0.375\n",
            "[Epoch: 10, Batch:   80 /  125], loss: 0.353\n",
            "[Epoch: 10, Batch:   90 /  125], loss: 0.431\n",
            "[Epoch: 10, Batch:  100 /  125], loss: 0.401\n",
            "[Epoch: 10, Batch:  110 /  125], loss: 0.382\n",
            "[Epoch: 10, Batch:  120 /  125], loss: 0.430\n",
            "Valid accuracy: 80 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "kSeYMnJdDQaQ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pointnet.eval();\n"
      ],
      "metadata": {
        "id": "ZqigmrnmiaHG"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        print('Batch [%4d / %4d]' % (i+1, len(valid_loader)))\n",
        "                   \n",
        "        inputs, labels = data['pointcloud'].float(), data['category']\n",
        "        outputs, __, __ = pointnet(inputs.transpose(1,2))\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        all_preds += list(preds.numpy())\n",
        "        all_labels += list(labels.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jUhWz2-ieNO",
        "outputId": "cdb8ce08-d4f0-40d9-ed35-0abb59c2274e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [   1 /   15]\n",
            "Batch [   2 /   15]\n",
            "Batch [   3 /   15]\n",
            "Batch [   4 /   15]\n",
            "Batch [   5 /   15]\n",
            "Batch [   6 /   15]\n",
            "Batch [   7 /   15]\n",
            "Batch [   8 /   15]\n",
            "Batch [   9 /   15]\n",
            "Batch [  10 /   15]\n",
            "Batch [  11 /   15]\n",
            "Batch [  12 /   15]\n",
            "Batch [  13 /   15]\n",
            "Batch [  14 /   15]\n",
            "Batch [  15 /   15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(all_labels, all_preds);\n",
        "cm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNM8FSW0iiPI",
        "outputId": "4527a4b1-2799-4d64-e77b-590fe88f2391"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0, 36,  0, 14,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 43,  0, 57,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 88,  0, 12,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 65,  0, 21,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 57,  0, 29,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 96,  0,  4,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 32,  0, 54,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 53,  0, 47,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 41,  0, 59,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 94,  0,  6,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}